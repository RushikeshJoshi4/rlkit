{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.mujoco import humanoid_v3\n",
    "\n",
    "import rlkit.torch.pytorch_util as ptu\n",
    "from rlkit.data_management.env_replay_buffer import EnvReplayBuffer\n",
    "from rlkit.envs.wrappers import NormalizedBoxEnv\n",
    "from rlkit.launchers.launcher_util import setup_logger\n",
    "from rlkit.samplers.data_collector import MdpPathCollector\n",
    "from rlkit.torch.sac.policies import TanhGaussianPolicy, MakeDeterministic\n",
    "from rlkit.torch.sac.sac import SACTrainer\n",
    "from rlkit.torch.networks import FlattenMlp\n",
    "from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm\n",
    "\n",
    "import abc\n",
    "from collections import OrderedDict\n",
    "\n",
    "import gtimer as gt\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from rlkit.core import logger, eval_util\n",
    "from rlkit.data_management.replay_buffer import ReplayBuffer\n",
    "from rlkit.samplers.data_collector import DataCollector\n",
    "\n",
    "import abc\n",
    "# import torch\n",
    "import gtimer as gt\n",
    "from rlkit.core.rl_algorithm import BaseRLAlgorithm\n",
    "from rlkit.data_management.replay_buffer import ReplayBuffer\n",
    "from rlkit.samplers.data_collector import PathCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import OrderedDict\n",
    "\n",
    "import gtimer as gt\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from rlkit.core import logger, eval_util\n",
    "from rlkit.data_management.replay_buffer import ReplayBuffer\n",
    "from rlkit.samplers.data_collector import DataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.launchers.launcher_util import run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = dict(\n",
    "        algorithm=\"SAC\",\n",
    "        version=\"normal\",\n",
    "        layer_size=256,\n",
    "        replay_buffer_size=int(1E6),\n",
    "        algorithm_kwargs=dict(\n",
    "            num_epochs=1,\n",
    "            num_eval_steps_per_epoch=5000,\n",
    "            num_trains_per_train_loop=1000,\n",
    "            num_expl_steps_per_train_loop=1000,\n",
    "            min_num_steps_before_training=1000,\n",
    "            max_path_length=1000,\n",
    "            batch_size=256,\n",
    "        ),\n",
    "        trainer_kwargs=dict(\n",
    "            discount=0.99,\n",
    "            soft_target_tau=5e-3,\n",
    "            target_update_period=1,\n",
    "            policy_lr=3E-4,\n",
    "            qf_lr=3E-4,\n",
    "            reward_scale=1,\n",
    "            use_automatic_entropy_tuning=True,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def _get_epoch_timings():\n",
    "    times_itrs = gt.get_times().stamps.itrs\n",
    "    times = OrderedDict()\n",
    "    epoch_time = 0\n",
    "    for key in sorted(times_itrs):\n",
    "        time = times_itrs[key][-1]\n",
    "        epoch_time += time\n",
    "        times['time/{} (s)'.format(key)] = time\n",
    "    times['time/epoch (s)'] = epoch_time\n",
    "    times['time/total (s)'] = gt.get_times().total\n",
    "    return times\n",
    "\n",
    "\n",
    "class BaseRLAlgorithm2(object, metaclass=abc.ABCMeta):\n",
    "    def __init__(\n",
    "            self,\n",
    "            trainer,\n",
    "            exploration_env,\n",
    "            evaluation_env,\n",
    "            exploration_data_collector: DataCollector,\n",
    "            evaluation_data_collector: DataCollector,\n",
    "            replay_buffer: ReplayBuffer,\n",
    "            initial_epoch\n",
    "    ):\n",
    "        self.trainer = trainer\n",
    "        self.expl_env = exploration_env\n",
    "        self.eval_env = evaluation_env\n",
    "        self.expl_data_collector = exploration_data_collector\n",
    "        self.eval_data_collector = evaluation_data_collector\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self._start_epoch = initial_epoch\n",
    "\n",
    "        self.post_epoch_funcs = []\n",
    "\n",
    "    def train(self, initial_epoch=0, epochs=None, dir_=None, exp_no=None):\n",
    "        self._train(initial_epoch, epochs, dir_, exp_no)\n",
    "\n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Train model.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('_train must implemented by inherited class')\n",
    "\n",
    "    def get_cur_best_metric_val(self):\n",
    "        cur_best_metric_val = None\n",
    "        if os.path.exists('results/humanoid/cur_best_avg_rewards.pkl'):\n",
    "            cur_best_metric_val = copy.deepcopy(torch.load('results/humanoid/cur_best_avg_rewards.pkl')['cur_best_metric_val'])\n",
    "        else:\n",
    "            cur_best_metric_val = -1* float('inf')\n",
    "        return cur_best_metric_val\n",
    "\n",
    "    def _end_epoch(self, epoch):\n",
    "        print('in _end_epoch, epoch is: {}'.format(epoch))\n",
    "        snapshot = self._get_snapshot()\n",
    "        logger.save_itr_params(epoch, snapshot)\n",
    "        # trainer_obj = self.trainer\n",
    "        # ckpt_path='ckpt.pkl'\n",
    "        # logger.save_ckpt(epoch, trainer_obj, ckpt_path)\n",
    "        # gt.stamp('saving')\n",
    "        if epoch%10==0:\n",
    "            self.save_snapshot_2(epoch)\n",
    "        eval_paths = self.eval_data_collector.get_epoch_paths()\n",
    "        d = eval_util.get_generic_path_information(eval_paths)\n",
    "        # print(d.keys())\n",
    "        metric_val = d['Returns Mean']\n",
    "        \n",
    "        cur_best_metric_val = self.get_cur_best_metric_val()\n",
    "        self.save_snapshot_2_best_only(metric_val=metric_val, cur_best_metric_val=cur_best_metric_val, min_or_max='max', epoch=epoch)\n",
    "        self._log_stats(epoch)\n",
    "\n",
    "        self.expl_data_collector.end_epoch(epoch)\n",
    "        self.eval_data_collector.end_epoch(epoch)\n",
    "        self.replay_buffer.end_epoch(epoch)\n",
    "        self.trainer.end_epoch(epoch)\n",
    "\n",
    "        for post_epoch_func in self.post_epoch_funcs:\n",
    "            post_epoch_func(self, epoch)\n",
    "\n",
    "    def save_snapshot_2(self, epoch):\n",
    "        print('Saving snapshot 2')\n",
    "        self_copy = copy.deepcopy(self)\n",
    "        torch.save(copy.deepcopy({'algorithm':self_copy, 'epoch':epoch}), 'results/humanoid/ckpt.pkl')\n",
    "\n",
    "    def get_snapshot_2(self):\n",
    "        print('in get_snapshot_2')\n",
    "        ckpt = {}\n",
    "        ckpt = torch.load('results/humanoid/ckpt.pkl')\n",
    "        self = copy.deepcopy(ckpt['algorithm'])\n",
    "        epoch = ckpt['epoch']\n",
    "        return epoch\n",
    "    \n",
    "    def get_snapshot_best(self):\n",
    "        print('in get_snapshot_best')\n",
    "        ckpt = None\n",
    "        ckpt = torch.load('results/humanoid/ckpt-best.pkl')\n",
    "        self = copy.deepcopy(ckpt['algorithm'])\n",
    "        epoch = ckpt['epoch']\n",
    "        return epoch\n",
    "\n",
    "    def save_snapshot_2_best_only(self, metric_val, cur_best_metric_val, min_or_max='min', epoch=0):\n",
    "        if min_or_max == 'min' and metric_val < cur_best_metric_val \\\n",
    "            or min_or_max == 'max' and metric_val > cur_best_metric_val:\n",
    "            print('Saving snapshot best')\n",
    "            print(metric_val)\n",
    "            print(cur_best_metric_val)\n",
    "            self_copy = copy.deepcopy(self)\n",
    "            torch.save({'algorithm':self_copy, 'epoch':epoch}, 'results/humanoid/ckpt-best.pkl')\n",
    "            cur_best_metric_val = metric_val\n",
    "            cur_best_metric_val_copy = copy.deepcopy(cur_best_metric_val)\n",
    "            torch.save({'cur_best_metric_val':cur_best_metric_val_copy}, 'results/humanoid/cur_best_avg_rewards.pkl')\n",
    "\n",
    "    # def _resume_training(self):\n",
    "\n",
    "    def _get_snapshot(self):\n",
    "        snapshot = {}\n",
    "        for k, v in self.trainer.get_snapshot().items():\n",
    "            snapshot['trainer/' + k] = v\n",
    "        for k, v in self.expl_data_collector.get_snapshot().items():\n",
    "            snapshot['exploration/' + k] = v\n",
    "        for k, v in self.eval_data_collector.get_snapshot().items():\n",
    "            snapshot['evaluation/' + k] = v\n",
    "        for k, v in self.replay_buffer.get_snapshot().items():\n",
    "            snapshot['replay_buffer/' + k] = v\n",
    "        return snapshot\n",
    "\n",
    "    def _log_stats(self, epoch):\n",
    "        logger.log(\"Epoch {} finished\".format(epoch), with_timestamp=True)\n",
    "\n",
    "        \"\"\"\n",
    "        Replay Buffer\n",
    "        \"\"\"\n",
    "        logger.record_dict(\n",
    "            self.replay_buffer.get_diagnostics(),\n",
    "            prefix='replay_buffer/'\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        Trainer\n",
    "        \"\"\"\n",
    "        logger.record_dict(self.trainer.get_diagnostics(), prefix='trainer/')\n",
    "\n",
    "        \"\"\"\n",
    "        Exploration\n",
    "        \"\"\"\n",
    "        logger.record_dict(\n",
    "            self.expl_data_collector.get_diagnostics(),\n",
    "            prefix='exploration/'\n",
    "        )\n",
    "        expl_paths = self.expl_data_collector.get_epoch_paths()\n",
    "        if hasattr(self.expl_env, 'get_diagnostics'):\n",
    "            logger.record_dict(\n",
    "                self.expl_env.get_diagnostics(expl_paths),\n",
    "                prefix='exploration/',\n",
    "            )\n",
    "        logger.record_dict(\n",
    "            eval_util.get_generic_path_information(expl_paths),\n",
    "            prefix=\"exploration/\",\n",
    "        )\n",
    "        \"\"\"\n",
    "        Evaluation\n",
    "        \"\"\"\n",
    "        logger.record_dict(\n",
    "            self.eval_data_collector.get_diagnostics(),\n",
    "            prefix='evaluation/',\n",
    "        )\n",
    "        eval_paths = self.eval_data_collector.get_epoch_paths()\n",
    "        if hasattr(self.eval_env, 'get_diagnostics'):\n",
    "            logger.record_dict(\n",
    "                self.eval_env.get_diagnostics(eval_paths),\n",
    "                prefix='evaluation/',\n",
    "            )\n",
    "        logger.record_dict(\n",
    "            eval_util.get_generic_path_information(eval_paths),\n",
    "            prefix=\"evaluation/\",\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        Misc\n",
    "        \"\"\"\n",
    "        # gt.stamp('logging')\n",
    "        logger.record_dict(_get_epoch_timings())\n",
    "        logger.record_tabular('Epoch', epoch)\n",
    "        logger.dump_tabular(with_prefix=False, with_timestamp=False, file_name='logs/humanoid/log1.txt', file_name2='logs/humanoid/log2')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def training_mode(self, mode):\n",
    "        \"\"\"\n",
    "        Set training mode to `mode`.\n",
    "        :param mode: If True, training will happen (e.g. set the dropout\n",
    "        probabilities to not all ones).\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import torch\n",
    "import gtimer as gt\n",
    "from rlkit.core.rl_algorithm import BaseRLAlgorithm\n",
    "from rlkit.data_management.replay_buffer import ReplayBuffer\n",
    "from rlkit.samplers.data_collector import PathCollector\n",
    "\n",
    "\n",
    "class BatchRLAlgorithm2(BaseRLAlgorithm2, metaclass=abc.ABCMeta):\n",
    "    def __init__(\n",
    "            self,\n",
    "            trainer,\n",
    "            exploration_env,\n",
    "            evaluation_env,\n",
    "            exploration_data_collector: PathCollector,\n",
    "            evaluation_data_collector: PathCollector,\n",
    "            replay_buffer: ReplayBuffer,\n",
    "            batch_size,\n",
    "            max_path_length,\n",
    "            num_epochs,\n",
    "            num_eval_steps_per_epoch,\n",
    "            num_expl_steps_per_train_loop,\n",
    "            num_trains_per_train_loop,\n",
    "            num_train_loops_per_epoch=1,\n",
    "            min_num_steps_before_training=0,\n",
    "            initial_epoch=0\n",
    "    ):\n",
    "        super().__init__(\n",
    "            trainer,\n",
    "            exploration_env,\n",
    "            evaluation_env,\n",
    "            exploration_data_collector,\n",
    "            evaluation_data_collector,\n",
    "            replay_buffer,\n",
    "            initial_epoch\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        self.max_path_length = max_path_length\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_eval_steps_per_epoch = num_eval_steps_per_epoch\n",
    "        self.num_trains_per_train_loop = num_trains_per_train_loop\n",
    "        self.num_train_loops_per_epoch = num_train_loops_per_epoch\n",
    "        self.num_expl_steps_per_train_loop = num_expl_steps_per_train_loop\n",
    "        self.min_num_steps_before_training = min_num_steps_before_training\n",
    "\n",
    "    # def store_everything(self, ckpt_path):\n",
    "    #     d = {\n",
    "            \n",
    "    #     }\n",
    "    #     torch.save(d, ckpt_path)\n",
    "\n",
    "    def _train(self, initial_epoch=0, epochs=None, dir_=None, exp_no=None):\n",
    "        \n",
    "        self._start_epoch = initial_epoch\n",
    "        if epochs is not None: \n",
    "            self.num_epochs = epochs\n",
    "        # print('\\n\\n\\n\\nn\\n\\n\\n\\nin _train #########')\n",
    "        if self.min_num_steps_before_training > 0:\n",
    "            init_expl_paths = self.expl_data_collector.collect_new_paths(\n",
    "                self.max_path_length,\n",
    "                self.min_num_steps_before_training,\n",
    "                discard_incomplete_paths=False,\n",
    "            )\n",
    "            self.replay_buffer.add_paths(init_expl_paths)\n",
    "            self.expl_data_collector.end_epoch(-1)\n",
    "         \n",
    "        # for epoch in gt.timed_for(\n",
    "        #         range(self._start_epoch, self.num_epochs),\n",
    "        #         save_itrs=True,\n",
    "        # ):\n",
    "\n",
    "        for epoch in range(self._start_epoch, self.num_epochs):\n",
    "\n",
    "            self.eval_data_collector.collect_new_paths(\n",
    "                self.max_path_length,\n",
    "                self.num_eval_steps_per_epoch,\n",
    "                discard_incomplete_paths=True,\n",
    "            )\n",
    "            # # gt.stamp('evaluation sampling')\n",
    "\n",
    "            for _ in range(self.num_train_loops_per_epoch):\n",
    "                new_expl_paths = self.expl_data_collector.collect_new_paths(\n",
    "                    self.max_path_length,\n",
    "                    self.num_expl_steps_per_train_loop,\n",
    "                    discard_incomplete_paths=False,\n",
    "                )\n",
    "                # # gt.stamp('exploration sampling', unique=False)\n",
    "\n",
    "                self.replay_buffer.add_paths(new_expl_paths)\n",
    "                # # gt.stamp('data storing', unique=False)\n",
    "\n",
    "                self.training_mode(True)\n",
    "                for _ in range(self.num_trains_per_train_loop):\n",
    "                    train_data = self.replay_buffer.random_batch(\n",
    "                        self.batch_size)\n",
    "                    self.trainer.train(train_data)\n",
    "                # # gt.stamp('training', unique=False)\n",
    "                self.training_mode(False)\n",
    "\n",
    "            self._end_epoch(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import OrderedDict\n",
    "\n",
    "from typing import Iterable\n",
    "from torch import nn as nn\n",
    "\n",
    "from rlkit.core.batch_rl_algorithm import BatchRLAlgorithm\n",
    "from rlkit.core.online_rl_algorithm import OnlineRLAlgorithm\n",
    "from rlkit.core.trainer import Trainer\n",
    "from rlkit.torch.core import np_to_pytorch_batch\n",
    "\n",
    "\n",
    "class TorchOnlineRLAlgorithm(OnlineRLAlgorithm):\n",
    "    def to(self, device):\n",
    "        for net in self.trainer.networks:\n",
    "            net.to(device)\n",
    "\n",
    "    def training_mode(self, mode):\n",
    "        for net in self.trainer.networks:\n",
    "            net.train(mode)\n",
    "\n",
    "\n",
    "class TorchBatchRLAlgorithm2(BatchRLAlgorithm2):\n",
    "\n",
    "    def to(self, device):\n",
    "        for net in self.trainer.networks:\n",
    "            net.to(device)\n",
    "\n",
    "    def training_mode(self, mode):\n",
    "        for net in self.trainer.networks:\n",
    "            net.train(mode)\n",
    "\n",
    "\n",
    "class TorchTrainer(Trainer, metaclass=abc.ABCMeta):\n",
    "    def __init__(self):\n",
    "        self._num_train_steps = 0\n",
    "\n",
    "    def train(self, np_batch):\n",
    "        self._num_train_steps += 1\n",
    "        batch = np_to_pytorch_batch(np_batch)\n",
    "        self.train_from_torch(batch)\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        return OrderedDict([\n",
    "            ('num train calls', self._num_train_steps),\n",
    "        ])\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train_from_torch(self, batch):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def networks(self) -> Iterable[nn.Module]:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snapshot_3():\n",
    "    print('in get_snapshot_3')\n",
    "    ckpt = {}\n",
    "    ckpt = torch.load('results/humanoid/ckpt.pkl')\n",
    "    # self = copy.deepcopy(ckpt['algorithm'])\n",
    "    epoch = ckpt['epoch']\n",
    "    return epoch, copy.deepcopy(ckpt['algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_function(variant):\n",
    "    setup_logger('experiment-5', variant=variant)\n",
    "    ptu.set_gpu_mode(True, 0)\n",
    "\n",
    "    expl_env = humanoid_v3.HumanoidEnv()\n",
    "    eval_env = humanoid_v3.HumanoidEnv()\n",
    "\n",
    "\n",
    "    expl_env = NormalizedBoxEnv(expl_env)\n",
    "    eval_env = NormalizedBoxEnv(eval_env)\n",
    "    obs_dim = expl_env.observation_space.low.size\n",
    "    action_dim = eval_env.action_space.low.size\n",
    "    # print(obs_dim, action_dim)\n",
    "    \n",
    "    M = variant['layer_size']\n",
    "    qf1 = FlattenMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    qf2 = FlattenMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    target_qf1 = FlattenMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    target_qf2 = FlattenMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    eval_policy = MakeDeterministic(policy)\n",
    "    eval_path_collector = MdpPathCollector(\n",
    "        eval_env,\n",
    "        eval_policy,\n",
    "    )\n",
    "    expl_path_collector = MdpPathCollector(\n",
    "        expl_env,\n",
    "        policy,\n",
    "    )\n",
    "    replay_buffer = EnvReplayBuffer(\n",
    "        variant['replay_buffer_size'],\n",
    "        expl_env,\n",
    "    )\n",
    "    trainer = SACTrainer(\n",
    "        env=eval_env,\n",
    "        policy=policy,\n",
    "        qf1=qf1,\n",
    "        qf2=qf2,\n",
    "        target_qf1=target_qf1,\n",
    "        target_qf2=target_qf2,\n",
    "        **variant['trainer_kwargs']\n",
    "    )\n",
    "    \n",
    "    resume = False\n",
    "    resume_from_best = False\n",
    "    algorithm = None\n",
    "    if not resume:\n",
    "        algorithm = TorchBatchRLAlgorithm2(\n",
    "            trainer=trainer,\n",
    "            exploration_env=expl_env,\n",
    "            evaluation_env=eval_env,\n",
    "            exploration_data_collector=expl_path_collector,\n",
    "            evaluation_data_collector=eval_path_collector,\n",
    "            replay_buffer=replay_buffer,\n",
    "            **variant['algorithm_kwargs']\n",
    "        )\n",
    "        algorithm.to(ptu.device)\n",
    "        algorithm.train(initial_epoch=0, epochs=4)\n",
    "    else:\n",
    "    #     algorithm = TorchBatchRLAlgorithm()\n",
    "    #     algorithm = TorchBatchRLAlgorithm2(\n",
    "    #         trainer=trainer,\n",
    "    #         exploration_env=expl_env,\n",
    "    #         evaluation_env=eval_env,\n",
    "    #         exploration_data_collector=expl_path_collector,\n",
    "    #         evaluation_data_collector=eval_path_collector,\n",
    "    #         replay_buffer=replay_buffer,\n",
    "    #         **variant['algorithm_kwargs']\n",
    "    #     )\n",
    "        if not resume_from_best:\n",
    "            initial_epoch, algorithm = get_snapshot_3()\n",
    "            initial_epoch+=1\n",
    "        else:\n",
    "            initial_epoch, algorithm = get_snapshot_best()\n",
    "            initial_epoch+=1\n",
    "        algorithm.to(ptu.device)\n",
    "        algorithm.train(initial_epoch=initial_epoch, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff467f20970>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doodad not set up! Running experiment here.\n",
      "setting seed to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rushikej/Desktop/tmp/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super init\n",
      "super done\n",
      "setting torch manual seed and numpy random seed here to 1\n",
      "assigning done\n",
      "calculating np.prod term\n",
      "thats done\n",
      "initializing alpha optimizer \n",
      "thats done\n",
      "auto entropy thing done\n",
      "criterions done\n",
      "optimizers done\n",
      "done\n",
      "in _end_epoch, epoch is: 0\n",
      "Saving snapshot 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rushikej/miniconda3/envs/tf_2/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type TanhGaussianPolicy. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rushikej/miniconda3/envs/tf_2/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rushikej/miniconda3/envs/tf_2/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlattenMlp. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rushikej/miniconda3/envs/tf_2/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MakeDeterministic. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rushikej/miniconda3/envs/tf_2/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _end_epoch, epoch is: 1\n",
      "in _end_epoch, epoch is: 2\n",
      "in _end_epoch, epoch is: 3\n"
     ]
    }
   ],
   "source": [
    "run_experiment(experiment_function, variant=variant,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm = TorchBatchRLAlgorithm2(\n",
    "#         trainer=trainer,\n",
    "#         exploration_env=expl_env,\n",
    "#         evaluation_env=eval_env,\n",
    "#         exploration_data_collector=expl_path_collector,\n",
    "#         evaluation_data_collector=eval_path_collector,\n",
    "#         replay_buffer=replay_buffer,\n",
    "#         **variant['algorithm_kwargs']\n",
    "#     )\n",
    "# algorithm_ = copy.deepcopy(algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch, algorithm = get_snapshot_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm.trainer.qf1.fcs[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm_.trainer.qf1.fcs[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm logs/humanoid/log2\n",
    "# !>logs/humanoid/log1.txt\n",
    "# !rm results/humanoid/*.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = [('replay_buffer/size', '2000'), ('trainer/QF1 Loss', '22.540974'), ('trainer/QF2 Loss', '22.461214'), ('trainer/Policy Loss', '-5.3794346'), ('trainer/Q1 Predictions Mean', '-0.013111563'), ('trainer/Q1 Predictions Std', '0.006633578'), ('trainer/Q1 Predictions Max', '0.004102228'), ('trainer/Q1 Predictions Min', '-0.031592093'), ('trainer/Q2 Predictions Mean', '-0.0048158974'), ('trainer/Q2 Predictions Std', '0.0035169804'), ('trainer/Q2 Predictions Max', '0.0017666357'), ('trainer/Q2 Predictions Min', '-0.018819787'), ('trainer/Q Targets Mean', '4.6183643'), ('trainer/Q Targets Std', '1.043318'), ('trainer/Q Targets Max', '8.902103'), ('trainer/Q Targets Min', '0.38821536'), ('trainer/Log Pis Mean', '-5.393486'), ('trainer/Log Pis Std', '0.62483275'), ('trainer/Log Pis Max', '-3.6818123'), ('trainer/Log Pis Min', '-6.818282'), ('trainer/Policy mu Mean', '-8.544093e-05'), ('trainer/Policy mu Std', '0.0021631739'), ('trainer/Policy mu Max', '0.006241211'), ('trainer/Policy mu Min', '-0.008601425'), ('trainer/Policy log std Mean', '0.00043238376'), ('trainer/Policy log std Std', '0.0024977052'), ('trainer/Policy log std Max', '0.007946691'), ('trainer/Policy log std Min', '-0.007701927'), ('trainer/Alpha', '0.9997000694274902'), ('trainer/Alpha Loss', '-0.0'), ('exploration/num steps total', '2000'), ('exploration/num paths total', '14'), ('exploration/path length Mean', '142.85714285714286'), ('exploration/path length Std', '203.16174314459656'), ('exploration/path length Max', '635'), ('exploration/path length Min', '24'), ('exploration/Rewards Mean', '-0.6064347764021665'), ('exploration/Rewards Std', '0.7633642575876102'), ('exploration/Rewards Max', '2.8795207953591153'), ('exploration/Rewards Min', '-3.6407089958048813'), ('exploration/Returns Mean', '-86.63353948602385'), ('exploration/Returns Std', '113.0231192346682'), ('exploration/Returns Max', '-3.105460048148942'), ('exploration/Returns Min', '-353.8074739814667'), ('exploration/Actions Mean', '-0.007008702'), ('exploration/Actions Std', '0.6308291'), ('exploration/Actions Max', '0.99908894'), ('exploration/Actions Min', '-0.9995207'), ('exploration/Num Paths', '7'), ('exploration/Average Returns', '-86.63353948602385'), ('exploration/env_infos/final/reward_forward Mean', '0.19075281818276574'), ('exploration/env_infos/final/reward_forward Std', '0.8342500879174919'), ('exploration/env_infos/final/reward_forward Max', '1.2204116659980113'), ('exploration/env_infos/final/reward_forward Min', '-1.1107150441758207'), ('exploration/env_infos/initial/reward_forward Mean', '-0.19105378778638557'), ('exploration/env_infos/initial/reward_forward Std', '0.1324312690431134'), ('exploration/env_infos/initial/reward_forward Max', '-0.00048826690484476964'), ('exploration/env_infos/initial/reward_forward Min', '-0.3908516428560994'), ('exploration/env_infos/reward_forward Mean', '-0.014457019663503719'), ('exploration/env_infos/reward_forward Std', '0.6431258664770597'), ('exploration/env_infos/reward_forward Max', '2.5457147574562833'), ('exploration/env_infos/reward_forward Min', '-2.3861966329817252'), ('exploration/env_infos/final/reward_ctrl Mean', '-1.5349954196384974'), ('exploration/env_infos/final/reward_ctrl Std', '0.15762081865148822'), ('exploration/env_infos/final/reward_ctrl Max', '-1.3189051151275635'), ('exploration/env_infos/final/reward_ctrl Min', '-1.7733919620513916'), ('exploration/env_infos/initial/reward_ctrl Mean', '-1.7483757819448198'), ('exploration/env_infos/initial/reward_ctrl Std', '0.4890421746133651'), ('exploration/env_infos/initial/reward_ctrl Max', '-0.8430193066596985'), ('exploration/env_infos/initial/reward_ctrl Min', '-2.4695911407470703'), ('exploration/env_infos/reward_ctrl Mean', '-1.5919777567386628'), ('exploration/env_infos/reward_ctrl Std', '0.43263451266003733'), ('exploration/env_infos/reward_ctrl Max', '-0.45637834072113037'), ('exploration/env_infos/reward_ctrl Min', '-2.8311009407043457'), ('exploration/env_infos/final/reward_contact Mean', '0.0'), ('exploration/env_infos/final/reward_contact Std', '0.0'), ('exploration/env_infos/final/reward_contact Max', '-0.0'), ('exploration/env_infos/final/reward_contact Min', '-0.0'), ('exploration/env_infos/initial/reward_contact Mean', '0.0'), ('exploration/env_infos/initial/reward_contact Std', '0.0'), ('exploration/env_infos/initial/reward_contact Max', '-0.0'), ('exploration/env_infos/initial/reward_contact Min', '-0.0'), ('exploration/env_infos/reward_contact Mean', '0.0'), ('exploration/env_infos/reward_contact Std', '0.0'), ('exploration/env_infos/reward_contact Max', '-0.0'), ('exploration/env_infos/reward_contact Min', '-0.0'), ('exploration/env_infos/final/reward_survive Mean', '1.0'), ('exploration/env_infos/final/reward_survive Std', '0.0'), ('exploration/env_infos/final/reward_survive Max', '1.0'), ('exploration/env_infos/final/reward_survive Min', '1.0'), ('exploration/env_infos/initial/reward_survive Mean', '1.0'), ('exploration/env_infos/initial/reward_survive Std', '0.0'), ('exploration/env_infos/initial/reward_survive Max', '1.0'), ('exploration/env_infos/initial/reward_survive Min', '1.0'), ('exploration/env_infos/reward_survive Mean', '1.0'), ('exploration/env_infos/reward_survive Std', '0.0'), ('exploration/env_infos/reward_survive Max', '1.0'), ('exploration/env_infos/reward_survive Min', '1.0'), ('exploration/env_infos/final/x_position Mean', '-0.10269478382571114'), ('exploration/env_infos/final/x_position Std', '1.0156116367327344'), ('exploration/env_infos/final/x_position Max', '0.9451816637118873'), ('exploration/env_infos/final/x_position Min', '-1.897179957278178'), ('exploration/env_infos/initial/x_position Mean', '-0.008983047047146756'), ('exploration/env_infos/initial/x_position Std', '0.055510131153156526'), ('exploration/env_infos/initial/x_position Max', '0.06390724254550409'), ('exploration/env_infos/initial/x_position Min', '-0.09279374271110855'), ('exploration/env_infos/x_position Mean', '0.6017503453335674'), ('exploration/env_infos/x_position Std', '0.5726451692146537'), ('exploration/env_infos/x_position Max', '1.427549938991314'), ('exploration/env_infos/x_position Min', '-1.897179957278178'), ('exploration/env_infos/final/y_position Mean', '0.4782543400247053'), ('exploration/env_infos/final/y_position Std', '0.7855700043684598'), ('exploration/env_infos/final/y_position Max', '1.550614516782081'), ('exploration/env_infos/final/y_position Min', '-1.0481446711405873'), ('exploration/env_infos/initial/y_position Mean', '0.004535862316807871'), ('exploration/env_infos/initial/y_position Std', '0.06869960071253207'), ('exploration/env_infos/initial/y_position Max', '0.10433425346335046'), ('exploration/env_infos/initial/y_position Min', '-0.09282306532235318'), ('exploration/env_infos/y_position Mean', '0.8093149087377335'), ('exploration/env_infos/y_position Std', '0.49822720634726986'), ('exploration/env_infos/y_position Max', '1.85534103577678'), ('exploration/env_infos/y_position Min', '-1.0481446711405873'), ('exploration/env_infos/final/distance_from_origin Mean', '1.2936266169235655'), ('exploration/env_infos/final/distance_from_origin Std', '0.4630235812512934'), ('exploration/env_infos/final/distance_from_origin Max', '2.007668134959712'), ('exploration/env_infos/final/distance_from_origin Min', '0.5422062892864513'), ('exploration/env_infos/initial/distance_from_origin Mean', '0.08576195513360897'), ('exploration/env_infos/initial/distance_from_origin Std', '0.023391580356425377'), ('exploration/env_infos/initial/distance_from_origin Max', '0.11835900951257046'), ('exploration/env_infos/initial/distance_from_origin Min', '0.04765527722351595'), ('exploration/env_infos/distance_from_origin Mean', '1.1705145479324601'), ('exploration/env_infos/distance_from_origin Std', '0.47237975365397095'), ('exploration/env_infos/distance_from_origin Max', '2.007668134959712'), ('exploration/env_infos/distance_from_origin Min', '0.026864676182351343'), ('exploration/env_infos/final/x_velocity Mean', '0.19075281818276574'), ('exploration/env_infos/final/x_velocity Std', '0.8342500879174919'), ('exploration/env_infos/final/x_velocity Max', '1.2204116659980113'), ('exploration/env_infos/final/x_velocity Min', '-1.1107150441758207'), ('exploration/env_infos/initial/x_velocity Mean', '-0.19105378778638557'), ('exploration/env_infos/initial/x_velocity Std', '0.1324312690431134'), ('exploration/env_infos/initial/x_velocity Max', '-0.00048826690484476964'), ('exploration/env_infos/initial/x_velocity Min', '-0.3908516428560994'), ('exploration/env_infos/x_velocity Mean', '-0.014457019663503719'), ('exploration/env_infos/x_velocity Std', '0.6431258664770597'), ('exploration/env_infos/x_velocity Max', '2.5457147574562833'), ('exploration/env_infos/x_velocity Min', '-2.3861966329817252'), ('exploration/env_infos/final/y_velocity Mean', '-0.40199064342737'), ('exploration/env_infos/final/y_velocity Std', '0.5971681663931471'), ('exploration/env_infos/final/y_velocity Max', '0.4077673083866995'), ('exploration/env_infos/final/y_velocity Min', '-1.259094084272049'), ('exploration/env_infos/initial/y_velocity Mean', '-0.0018872918578139997'), ('exploration/env_infos/initial/y_velocity Std', '0.14466164945930343'), ('exploration/env_infos/initial/y_velocity Max', '0.18527643991947776'), ('exploration/env_infos/initial/y_velocity Min', '-0.24354054905268124'), ('exploration/env_infos/y_velocity Mean', '0.06630737583610093'), ('exploration/env_infos/y_velocity Std', '0.5814660994205909'), ('exploration/env_infos/y_velocity Max', '2.4733737635902187'), ('exploration/env_infos/y_velocity Min', '-1.7773695514104038'), ('exploration/env_infos/final/forward_reward Mean', '0.19075281818276574'), ('exploration/env_infos/final/forward_reward Std', '0.8342500879174919'), ('exploration/env_infos/final/forward_reward Max', '1.2204116659980113'), ('exploration/env_infos/final/forward_reward Min', '-1.1107150441758207'), ('exploration/env_infos/initial/forward_reward Mean', '-0.19105378778638557'), ('exploration/env_infos/initial/forward_reward Std', '0.1324312690431134'), ('exploration/env_infos/initial/forward_reward Max', '-0.00048826690484476964'), ('exploration/env_infos/initial/forward_reward Min', '-0.3908516428560994'), ('exploration/env_infos/forward_reward Mean', '-0.014457019663503719'), ('exploration/env_infos/forward_reward Std', '0.6431258664770597'), ('exploration/env_infos/forward_reward Max', '2.5457147574562833'), ('exploration/env_infos/forward_reward Min', '-2.3861966329817252'), ('evaluation/num steps total', '5000'), ('evaluation/num paths total', '5'), ('evaluation/path length Mean', '1000.0'), ('evaluation/path length Std', '0.0'), ('evaluation/path length Max', '1000'), ('evaluation/path length Min', '1000'), ('evaluation/Rewards Mean', '0.9983870785388878'), ('evaluation/Rewards Std', '0.04913844925352009'), ('evaluation/Rewards Max', '1.9170770036140046'), ('evaluation/Rewards Min', '-0.13733900279654443'), ('evaluation/Returns Mean', '998.3870785388877'), ('evaluation/Returns Std', '3.6701104981273907'), ('evaluation/Returns Max', '1004.5377664821074'), ('evaluation/Returns Min', '994.229294090189'), ('evaluation/Actions Mean', '-3.18751e-05'), ('evaluation/Actions Std', '0.0010393225'), ('evaluation/Actions Max', '0.0051202956'), ('evaluation/Actions Min', '-0.0029151689'), ('evaluation/Num Paths', '5'), ('evaluation/Average Returns', '998.3870785388877'), ('evaluation/env_infos/final/reward_forward Mean', '0.0006460700812505518'), ('evaluation/env_infos/final/reward_forward Std', '0.000683088293812967'), ('evaluation/env_infos/final/reward_forward Max', '0.0015115855220859675'), ('evaluation/env_infos/final/reward_forward Min', '-6.473937490869552e-05'), ('evaluation/env_infos/initial/reward_forward Mean', '-0.03758082171038936'), ('evaluation/env_infos/initial/reward_forward Std', '0.12706800538907964'), ('evaluation/env_infos/initial/reward_forward Max', '0.10714505461736121'), ('evaluation/env_infos/initial/reward_forward Min', '-0.22169929465191252'), ('evaluation/env_infos/reward_forward Mean', '-0.001608596634589209'), ('evaluation/env_infos/reward_forward Std', '0.04913838565616579'), ('evaluation/env_infos/reward_forward Max', '0.9170830225171546'), ('evaluation/env_infos/reward_forward Min', '-1.13733215985436'), ('evaluation/env_infos/final/reward_ctrl Mean', '-4.327109218138503e-06'), ('evaluation/env_infos/final/reward_ctrl Std', '2.26174565875007e-08'), ('evaluation/env_infos/final/reward_ctrl Max', '-4.29289639214403e-06'), ('evaluation/env_infos/final/reward_ctrl Min', '-4.362076651887037e-06'), ('evaluation/env_infos/initial/reward_ctrl Mean', '-3.2657525480317417e-06'), ('evaluation/env_infos/initial/reward_ctrl Std', '2.1728433187299645e-07'), ('evaluation/env_infos/initial/reward_ctrl Max', '-2.892852990044048e-06'), ('evaluation/env_infos/initial/reward_ctrl Min', '-3.553457645466551e-06'), ('evaluation/env_infos/reward_ctrl Mean', '-4.3248265230431574e-06'), ('evaluation/env_infos/reward_ctrl Std', '8.365455465240826e-07'), ('evaluation/env_infos/reward_ctrl Max', '-2.892852990044048e-06'), ('evaluation/env_infos/reward_ctrl Min', '-2.5335128157166764e-05'), ('evaluation/env_infos/final/reward_contact Mean', '0.0'), ('evaluation/env_infos/final/reward_contact Std', '0.0'), ('evaluation/env_infos/final/reward_contact Max', '-0.0'), ('evaluation/env_infos/final/reward_contact Min', '-0.0'), ('evaluation/env_infos/initial/reward_contact Mean', '0.0'), ('evaluation/env_infos/initial/reward_contact Std', '0.0'), ('evaluation/env_infos/initial/reward_contact Max', '-0.0'), ('evaluation/env_infos/initial/reward_contact Min', '-0.0'), ('evaluation/env_infos/reward_contact Mean', '0.0'), ('evaluation/env_infos/reward_contact Std', '0.0'), ('evaluation/env_infos/reward_contact Max', '-0.0'), ('evaluation/env_infos/reward_contact Min', '-0.0'), ('evaluation/env_infos/final/reward_survive Mean', '1.0'), ('evaluation/env_infos/final/reward_survive Std', '0.0'), ('evaluation/env_infos/final/reward_survive Max', '1.0'), ('evaluation/env_infos/final/reward_survive Min', '1.0'), ('evaluation/env_infos/initial/reward_survive Mean', '1.0'), ('evaluation/env_infos/initial/reward_survive Std', '0.0'), ('evaluation/env_infos/initial/reward_survive Max', '1.0'), ('evaluation/env_infos/initial/reward_survive Min', '1.0'), ('evaluation/env_infos/reward_survive Mean', '1.0'), ('evaluation/env_infos/reward_survive Std', '0.0'), ('evaluation/env_infos/reward_survive Max', '1.0'), ('evaluation/env_infos/reward_survive Min', '1.0'), ('evaluation/env_infos/final/x_position Mean', '-0.08246357007500449'), ('evaluation/env_infos/final/x_position Std', '0.21278604707291804'), ('evaluation/env_infos/final/x_position Max', '0.2762546681301775'), ('evaluation/env_infos/final/x_position Min', '-0.31765979047571263'), ('evaluation/env_infos/initial/x_position Mean', '-0.003912779431063491'), ('evaluation/env_infos/initial/x_position Std', '0.0330324112758067'), ('evaluation/env_infos/initial/x_position Max', '0.05434083206249111'), ('evaluation/env_infos/initial/x_position Min', '-0.03564545654418633'), ('evaluation/env_infos/x_position Mean', '-0.0980639978100126'), ('evaluation/env_infos/x_position Std', '0.2219732113910267'), ('evaluation/env_infos/x_position Max', '0.28447775987369606'), ('evaluation/env_infos/x_position Min', '-0.4154507641762448'), ('evaluation/env_infos/final/y_position Mean', '-0.21619497009687377'), ('evaluation/env_infos/final/y_position Std', '0.11467158032691588'), ('evaluation/env_infos/final/y_position Max', '-0.06849864226409814'), ('evaluation/env_infos/final/y_position Min', '-0.3834214088146994'), ('evaluation/env_infos/initial/y_position Mean', '0.03461183079311117'), ('evaluation/env_infos/initial/y_position Std', '0.03786474659873482'), ('evaluation/env_infos/initial/y_position Max', '0.0727745156466771'), ('evaluation/env_infos/initial/y_position Min', '-0.031890929939110295'), ('evaluation/env_infos/y_position Mean', '-0.18847055253266243'), ('evaluation/env_infos/y_position Std', '0.12900336667512582'), ('evaluation/env_infos/y_position Max', '0.11318113181673488'), ('evaluation/env_infos/y_position Min', '-0.3843479847936035'), ('evaluation/env_infos/final/distance_from_origin Mean', '0.31742811785734604'), ('evaluation/env_infos/final/distance_from_origin Std', '0.10586486027873725'), ('evaluation/env_infos/final/distance_from_origin Max', '0.41272207970165087'), ('evaluation/env_infos/final/distance_from_origin Min', '0.11858511872844475'), ('evaluation/env_infos/initial/distance_from_origin Mean', '0.05664015906534127'), ('evaluation/env_infos/initial/distance_from_origin Std', '0.023023038124026837'), ('evaluation/env_infos/initial/distance_from_origin Max', '0.090824314788785'), ('evaluation/env_infos/initial/distance_from_origin Min', '0.020441331046195648'), ('evaluation/env_infos/distance_from_origin Mean', '0.31550155666738033'), ('evaluation/env_infos/distance_from_origin Std', '0.10728671757338556'), ('evaluation/env_infos/distance_from_origin Max', '0.41675052787889233'), ('evaluation/env_infos/distance_from_origin Min', '0.006118439492803657'), ('evaluation/env_infos/final/x_velocity Mean', '0.0006460700812505518'), ('evaluation/env_infos/final/x_velocity Std', '0.000683088293812967'), ('evaluation/env_infos/final/x_velocity Max', '0.0015115855220859675'), ('evaluation/env_infos/final/x_velocity Min', '-6.473937490869552e-05'), ('evaluation/env_infos/initial/x_velocity Mean', '-0.03758082171038936'), ('evaluation/env_infos/initial/x_velocity Std', '0.12706800538907964'), ('evaluation/env_infos/initial/x_velocity Max', '0.10714505461736121'), ('evaluation/env_infos/initial/x_velocity Min', '-0.22169929465191252'), ('evaluation/env_infos/x_velocity Mean', '-0.001608596634589209'), ('evaluation/env_infos/x_velocity Std', '0.04913838565616579'), ('evaluation/env_infos/x_velocity Max', '0.9170830225171546'), ('evaluation/env_infos/x_velocity Min', '-1.13733215985436'), ('evaluation/env_infos/final/y_velocity Mean', '-0.00019060288440309048'), ('evaluation/env_infos/final/y_velocity Std', '0.0006822993069578147'), ('evaluation/env_infos/final/y_velocity Max', '0.0003515250043489848'), ('evaluation/env_infos/final/y_velocity Min', '-0.0015195556259323117'), ('evaluation/env_infos/initial/y_velocity Mean', '0.021937314969317126'), ('evaluation/env_infos/initial/y_velocity Std', '0.08664719085850509'), ('evaluation/env_infos/initial/y_velocity Max', '0.15565432328174394'), ('evaluation/env_infos/initial/y_velocity Min', '-0.08328165113421354'), ('evaluation/env_infos/y_velocity Mean', '-0.004994198702830382'), ('evaluation/env_infos/y_velocity Std', '0.04931474746785201'), ('evaluation/env_infos/y_velocity Max', '0.8030629918524109'), ('evaluation/env_infos/y_velocity Min', '-1.2179716906352427'), ('evaluation/env_infos/final/forward_reward Mean', '0.0006460700812505518'), ('evaluation/env_infos/final/forward_reward Std', '0.000683088293812967'), ('evaluation/env_infos/final/forward_reward Max', '0.0015115855220859675'), ('evaluation/env_infos/final/forward_reward Min', '-6.473937490869552e-05'), ('evaluation/env_infos/initial/forward_reward Mean', '-0.03758082171038936'), ('evaluation/env_infos/initial/forward_reward Std', '0.12706800538907964'), ('evaluation/env_infos/initial/forward_reward Max', '0.10714505461736121'), ('evaluation/env_infos/initial/forward_reward Min', '-0.22169929465191252'), ('evaluation/env_infos/forward_reward Mean', '-0.001608596634589209'), ('evaluation/env_infos/forward_reward Std', '0.04913838565616579'), ('evaluation/env_infos/forward_reward Max', '0.9170830225171546'), ('evaluation/env_infos/forward_reward Min', '-1.13733215985436'), ('time/epoch (s)', '0'), ('time/total (s)', '70.16498186439276'), ('Epoch', '0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ll in l:\n",
    "#     print(ll[0]+'\\t'+ll[1])\n",
    "#     print(ll[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rlkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1 = {}\n",
    "# i1 = 0\n",
    "# for line in rlkit.core.tabulate.tabulate(l).split('\\n'):\n",
    "#     print(line)\n",
    "#     print(i1)\n",
    "#     print(l[i1][0])\n",
    "#     print(l[i1][1])\n",
    "#     d1[l[i1][0]] = l[i1][1]; i1+=1\n",
    "# #     self.log(line, *args, **kwargs, file_name=file_name)\n",
    "# d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm *.pkl logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !ls | grep pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat logs/2020-04-13\\ 18\\:51\\:02.545425.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('temp3', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.write('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm temp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load('ckpt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm = ckpt['algorithm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_ = 'results/humanoid/'\n",
    "# f1 = dir_ + 'ckpt.pkl'\n",
    "# f2 = dir_ + 'ckpt-best.pkl'\n",
    "# x1 = torch.load(f1)\n",
    "# x2 = torch.load(f2)\n",
    "# x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1['algorithm'].trainer.qf1.fc1 == x2['algorithm'].trainer.qf1.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc1 = x1['algorithm'].trainer.qf1.fcs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc2 = x2['algorithm'].trainer.qf1.fcs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc2.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm.train(initial_epoch=initial_epoch, epochs = 2502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_ = 'ant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = None\n",
    "# with open('results/'+type_+'/tmp3/ckpt.pkl', 'rb') as f:\n",
    "#     a = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = None\n",
    "# with open('results/'+type_+'/ckpt.pkl', 'rb') as f:\n",
    "#     b = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a['algorithm'].trainer.qf1.fcs[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = a['algorithm']\n",
    "# aa2 = copy.deepcopy(aa)\n",
    "# bb = b['algorithm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa.trainer.use_automatic_entropy_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e, aa3 = get_snapshot_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_snapshot_3():\n",
    "#     print('in get_snapshot_3')\n",
    "#     ckpt = {}\n",
    "#     ckpt = torch.load('results/humanoid/ckpt.pkl')\n",
    "#     # self = copy.deepcopy(ckpt['algorithm'])\n",
    "#     epoch = ckpt['epoch']\n",
    "#     return epoch, copy.deepcopy(ckpt['algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_2] *",
   "language": "python",
   "name": "conda-env-tf_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
